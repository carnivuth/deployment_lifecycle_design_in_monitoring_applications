\chapter{Progettazione della soluzione}
%Progettazione della architettura di produzione del software applicativo, gestione di tutti i componenti critici per la corretta erogazione dei servizi offerti e progettazione del sistema di software management per la gestione del life-cycle, progettazione di tutte le operazioni necessarie per la migrazione alla nuova architettura

Dato il contesto descritto sopra uno dei sotto obbiettivi risultava essere l'allineamento dell'infrastruttura di sanet a una sola architettura comune, che semplificasse l'installazione, e l'aggiunta di componenti a sanet, i requisiti di questa infrastruttura sono stati:

\begin{itemize}
\item{riproducibilità della stessa}
\item{versioning dell'infrastruttura}
\item{possibilità di gestione e monitoraggio centralizzati}
\end{itemize}
% Il problema del deployment
Il problema principale riscontrato in fase di analisi e quello legato alla gestione del deployment da parte dei due team coinvolti NAD e NMS, nel workflow mostrato la progettazione del deployment e delegata ai team sistemistici, impedendo all'area NAD di conoscere lo stato delle istanze in produzione, inoltre l'assenza di artefatti prodotti fa si che fra la progettazione del deployment e le effettive operazioni di messa in produzione non esista un interfaccia chiara che costringa le due parti a un workflow condiviso.

Risultava quindi necessario progettare un artefatto e la conseguente procedura di build che dati i sorgenti di sanet producesse un oggetto installabile, ma che allo stesso tempo lasciasse ai team sistemistici la possibilità di modificare l'ambiente di esecuzione in caso di necessità per rispondere tempestivamente alle richieste dei clienti.

\newpage
\subsection{Progettazione dell'artefatto: Il container}

La necessità di pacchettizzare sia le dipendenze applicative che demoni come redis e postgres ha fatto si che il team optasse per una soluzione basata su container.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/container.png}
    \caption{container come artefatto}
    \label{fig:enter-label}
\end{figure}

% LXC come soluzione a container
Per la creazione del container il team ha optato per la tecnologia LXC\cite{LXC} che offre un set di API di basso livello per la creazione di ambienti runtime isolati sfruttando le possibilità offerte dalla funzionalità namespaces del kernel linux. La tecnologia LXC non fa assunzioni sulla natura del container e consente la creazione di ambienti virtuali completi di init system dove all'interno possono risiedere processi demoni di varia natura.

% struttura del container LXC
Nel container LXC vengono predisposti tutti i moduli di sanet e viene creato il virtual environment di python necessario per l'esecuzione, vengono inoltre installati tutti i demoni necessari al funzionamento di sanet, e inoltre presente il software rancid per il backup periodico delle configurazioni degli apparati e la sua integrazione con sanet

% gestione dei dati
Sanet e rancid producono una mole di dati proporzionale al numero di datagroup monitorati di cui viene mantenuto uno storico di un anno, in un installazione medio piccola questo comporta \(\simeq 1000 GB\) di occupazione di storage, questi dati inoltre devono essere sottoposti a backup e sopravvivere a una distruzione del container, per garantirlo viene sfruttata la tecnologia bind mount offerta da LXC\cite{LXC}
I dati prodotti da sanet e rancid vengono esportati sulla macchina host per mezzo di bind-mount.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/container_data_management.png}
    \caption{gestione dei dati e bind mounts}
    \label{fig:enter-label}
\end{figure}

% gestione della network
Il container di sanet fa parte di una rete privata con indirizzamento \verb|169.254.254.x/24|, il demone apache è esposto all' esterno per mezzo di un reverse proxy HTTP che inoltra le richieste verso il container, in caso di presenza di agenti remoti, viene esposto anche redis per mezzo di un port forwarding in modo da consentire la comunicazione con macchine esterne

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{build/container_network.png}
    \caption{networking nel container}
    \label{fig:enter-label}
\end{figure}

% gestione delle integrazioni
Una grande sfida per il progetto e stato determinare una politica con cui gestire le integrazioni, non era possibile ignorarle in fase di progettazione in quanto avrebbe comportato un grave disservizio per il cliente e la re-ingegnerizzazione di ognuna di queste avrebbe comportato un aumento non accettabile dei tempi di sviluppo del progetto, il team ha dunque optato per una riorganizzazione delle integrazioni sistemistiche più frequenti in una raccolta pre installata nel container, questa include:

\begin{itemize}
  \item{discovery automatica dei nodi da aggiungere a monitoraggio}
  \item{backup dei dati applicativi}
  \item{sistema di email spia per verificare il corretto funzionamento della posta}
  \item{tools per il sincronismo di installazioni master-slave}
  \item{monitoraggio di infrastrutture checkmk}
  \item{monitoraggio di infrastrutture VMware}
  \item{monitoraggio del servizio radius }
  \item{monitoraggio del servizio ldap }
  \item{monitoraggio del servizio samba }
\end{itemize}

Non tutte le integrazioni sono presenti all'interno del container di default di conseguenza il container e predisposto per preservare le integrazioni esistenti che vengono importate nel container per mezzo di un bind mount

% gestione delle configurazioni
Le installazioni in produzione prevedono un set di configurazioni custom degli applicativi a bordo come per esempio la configurazione di sanet stesso, del demone di posta, di rancid oppure di postgres, l'architettura a container doveva tener conto di queste configurazioni e riportarle a seguito di una migrazione, per soddisfare il requisito si è deciso di esportare le configurazioni per mezzo di repository \verb|git|

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/customer_repository.png}
    \caption{gestione delle configurazioni per mezzo di git}
    \label{fig:enter-label}
\end{figure}

Viene predisposta una repository per cliente per garantire la divisione di ambito e all' interno viene replicata la struttura \verb|FHS| con all'interno le configurazioni necessarie ai servizi presenti nel container per funzionare correttamente, Per le prime installazioni e inoltre prevista una repository di configurazioni di default che consenta ai servizi di partire da cui viene creata la versione specifica per il cliente in questione

% perché il container bulky
Per evitare di modificare quanto sviluppato dall'area NAD il container in questione e pensato come architettura all-in-one che pacchettizza tutti i demoni necessari al sistema, in questo modo lo sviluppo di sanet può essere organizzato in maniera indipendente a quello degli artefatti

% perché non docker
Sono state valutate anche soluzioni che prevedessero l'uso della tecnologia docker\cite{docker}, queste tuttavia limitavano molto il workflow dei team sistemistici che non avrebbero potuto modificare agilmente il container in caso di necessità.

\newpage
\subsection{Progettazione del provision: Ansible}

Progettare l'artefatto non e sufficiente, e necessario prevedere le procedure che portino il container in produzione, ne effettuino l'aggiornamento e gestiscano il monitoraggio delle istanze stesse, per adempiere all'obbiettivo il team ha previsto un'architettura centralizzata in cui una macchina provisioner si occupa di gestire lo stato dell'installato, questa viene amministrata dal team stesso.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/provisioner_architecture.png}
    \caption{architettura del provisioner}
    \label{fig:enter-label}
\end{figure}

Il provisioner e si occupa dei seguenti compiti:

\begin{itemize}
  \item{creare l'artefatto installabile}
  \item{installare istanza di sanet su infrastruttura on premise }
  \item{aggiornare le istanze di sanet}
  \item{aggiungere al monitoraggio centralizzato un istanza di sanet}
  \item{effettuare operazioni massive come riconfigurazione di componenti dell'infrastruttura applicativa}
\end{itemize}

Il team ha deciso di optare per ansible\cite{ansible} come tecnologia di riferimento per effettuare il provisioning, le procedure vengono implementate come ansible playbooks e il nodo provisioner viene implementato come ansible tower

% progettazione della procedura di build
L'artefatto viene ricreato per mezzo di un playbook che effettua la build del container partendo da uno scheletro base minimale e installando le componenti di necessarie, la build viene controllata dal provisioner e effettuata su una macchina di staging, consentendo al team di effettuare test manuali con l'obbiettivo di collaudare il funzionamento delle sue componenti e integrazioni aggiuntive, nonché testare le nuove features introdotte da un aggiornamento

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/build_procedure.png}
    \caption{procedura di build}
    \label{fig:enter-label}
\end{figure}

La procedura di build consente di ottenere in maniera programmatica un infrastruttura identica per ogni installazione, e rende possibile la progettazione di operazioni massive in quanto si possono fare assunzioni sull'ambiente runtime dove si va ad operare, questo risulta essere un requisito fondamentale per rendere la manutenzione delle installazioni scalabile

% progettazione della procedura di installazione
Gli artefatti prodotti dalla procedura di build devono poter essere installati in maniera programmatica, è stata dunque prevista una procedura di di installazione di una nuova istanza di sanet a partire dagli artefatti prodotti dalla procedura di build, l'installazione prevede 3 fasi principali:

\begin{itemize}
  \item{preparazione della macchina host e installazione delle componenti principali}
  \item{ottenimento dell'artefatto}
  \item{configurazione dell'artefatto con le configurazioni del cliente specifico}
\end{itemize}

In questa fase viene effettuata inoltre l'inizializzazione del database postgres, necessaria in quanto in fase di installazione la directory dati di postgres viene montata all'esterno del container provocandone la cancellazione dei contenuti, vengono anche inizializzati

% implementazione con ansible
Il playbook che implementa la procedura di installazione e strutturato secondo il pattern strategy, il playbook specifica le macchine in cui eseguire la procedura e richiama un role ansible comportandosi da proxy, è il role stesso che implementa le logiche operative di installazione dello specifico contianer.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/proxy_playbook.png}
    \caption{procedura di build}
    \label{fig:enter-label}
\end{figure}

In questo modo il playbook di installazione può essere utilizzato anche in fase di build per effettuare il deploy del container skel

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{build/build_deploy.png}
    \caption{relazione tra build e installazione}
    \label{fig:enter-label}
\end{figure}

% progettazione della procedura di migrazione
Come detto in precedenza lo stato dell'installato non poteva essere ignorato in questa fase, era fondamentale prevedere una procedura di migrazione che dato uno stato dell'installato potenzialmente ignoto fino alla fase di attuazione della procedura stessa, il team ha adottato un approccio conservativo conscio dei possibili problemi tra cui

\begin{itemize}
  \item{clashing di indirizzamenti ip con quelli del container}
  \item{clashing di nomi all'interno del filesystem}
  \item{stato dei dischi}
  \item{configurazioni discordanti tra il container e quanto presente in produzione}
\end{itemize}

% operazioni pre migrazione
In particolare in questa fase era necessario reperire le configurazioni custom della istanza in questione per poter riconfigurare l'artefatto successivamente, e stato dunque previsto un playbook che si occupasse di creare la repository git e esportare le configurazioni interessanti della istanza,vengono inoltre apportate delle correzioni alle configurazioni per quei software che a seguito di aggiornamenti hanno deprecato dei parametri di configurazione, in caso non sia presente una specifica configurazione viene fornito il default previsto dal team per quel particolare pezzo di software.

Per accelerare la fase di migrate e ridurre il conseguente downtime viene effettuata una pulizia  dei log salvati nel database in modo da ridurre la quantità di dati da esportare.

La procedura di migrate completa si compone delle seguenti fasi

\begin{itemize}
  \item{esportazione delle configurazioni per mezzo di git}
  \item{pulizia dei log a database della istanza in produzione}
  \item{creazione di una istanza di sanet temporanea a bordo della infrastruttura del cliente}
  \item{arresto della vecchia istanza di sanet}
  \item{dump del database}
  \item{restore del database nella nuova infrastruttura a container}
  \item{riconfigurazione della nuova istanza per mezzo della repository git}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/migration.png}
    \caption{procedura di migrazione}
    \label{fig:enter-label}
\end{figure}

% progettazione della procedura di update
Una volta migrate le istanze in produzione era necessario prevedere un workflow che tenesse le istanze aggiornate, la progettazione della procedura di update ha comportato due principali problematiche, la prima di natura tecnica in quanto le installazioni di sanet risultano comunque molto diverse in termini di nodi coinvolti e loro natura, per esempio nel cliente \(\alpha\) dopo la migrazione l'infrastruttura presentava lo schema master slave visto in precedenza

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/alpha_client.png}
    \caption{infrastruttura cliente alpha}
    \label{fig:enter-label}
\end{figure}

In queste situazioni vi e la possibilita di testare l'update sfruttando l'installazione slave replicando uno schema di aggiornamento A/B, dove si aggiorna prima l'istanza non utilizzata per verificare l'impatto delle modifiche su un infrastruttura il piu vicino possibile a quella di produzione, tuttavia esistono anche le installazioni di classe \(\beta\)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{build/beta_client.png}
    \caption{infrastruttura cliente beta}
    \label{fig:enter-label}
\end{figure}

In queste infrastrutture non e prevista un istanza di backup o l'istanza di backup non replica perfettamente l'istanza master qui lo schema di aggiornamento A/B non risulta ugualmente utile o nel peggiore dei casi non e attuabile, la terza tipologia di istanze e quella \(\gamma\)

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/gamma_client.png}
    \caption{infrastruttura cliente gamma}
    \label{fig:enter-label}
\end{figure}

Queste sono le infrastrutture più complesse, presentano nodi che spesso sono distribuiti in locazioni geografiche remote, macchine fisiche che presentano elementi infrastrutturali sia della installazione master che slave e logiche di failover implementate a livello del singolo elemento infrastrutturale.

Un'altra importante considerazione da fare riguarda le installazioni per cui e previsto servizio di reperibilità, il loro funzionamento risulta essere più critico delle altre.

Inoltre la procedura di update ha come requisito quello di generare il downtime minimo possibile in fase di aggiornamento e accomodare tutti i possibili scenari di update tra cui:

\begin{itemize}
  \item{aggiornamento alla webui di sanet}
  \item{aggiornamento delle dipendenze di sanet}
  \item{aggiornamento di un sorgente di sanet}
  \item{aggiornamento delle integrazioni}
\end{itemize}

Va inoltre tenuto in considerazione che i nodi che ospitano agenti remoti devonon essere aggiornati contestualmente al nodo sanetd di riferimento in quanto l'area NAD non garantisce la compatibilità del protocollo di comunicazione tra diverse versioni del demone principale e degli agenti remoti

Per affrontare il problema la procedura di update e stata suddivisa in 4 batch di aggiornamento, ogni batch contiene un sottoinsieme delle istanze di sanet e le istanze che fanno parte dello stesso batch vengono aggiornate insieme, dopo l'esecuzione di un batch le istanze che ne fanno parte sono tenute sotto osservazione in modo da identificare problemi prima di aggiornare il batch successivo

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/update_batches.png}
    \caption{procedura di aggiornamento di sanet in batches}
    \label{fig:enter-label}
\end{figure}

Problema non banale è stato quello di suddividere le istanze nei vari batches, per far cio e stata definita una funzione di scorig \(S\) che valutasse la criticita di un installazione in base ai seguenti parametri:

\begin{itemize}
  \item{numero di nodi monitorati dall'istanza}
  \item{numero di nodi appartenenti al cluster di monitoraggio}
  \item{se l'istanza e slave}
  \item{se l'istanza fornisce servizio h24}
\end{itemize}

\newpage
La funzione di scoring aveva inoltre come vincolo che data una installazione slave e la corrisponende master si avesse che \(batch(slave_instance) \lt batch(master_instance)\)

\begin{lstlisting}[language=python]
def compute_critical(hosts,host):
    repcoeff = 0.8 if 'rep' in host and host["rep"] else 0.2
    rolecoeff = 0.8 if 'sanet' in host["containers"] and host["containers"]["sanet"]["role"] == "prod" else 0.2
    clustercoeff = 0.8*cluster_nodes(hosts,host)
    num_nodes = host["containers"]["sanet"]["nodes"] if "nodes" in host["containers"]["sanet"] else 1
    return repcoeff * rolecoeff * clustercoeff * num_nodes
\end{lstlisting}

Per poter ridurre il downtime in fase di update l'implementazione della procedura e stata pensata per essere personalizzabile in fase di esecuzione, in qunanto differenti aggiornamenti possono comportare il riavvio di componenti diversi della infrastruttura, per esempio in caso di aggiornamento alla web ui non e necessario riavviare il demone principale sanetd o aggiornare gli eventuali poller remoti

% monitoraggio delle istanze centralizzato
Le istanze di sanet aggiornate necessitano di monitoraggio per garantire il mantenimento dello stato dell'installato, per far cio si è predisposto un istanza di sanet centrale che accentrasse le informazioni di stato dell'installato, queste vengono raccolte per mezzo di un endpoint HTTP esposto dalle singole istanze di sanet

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{build/sanet_monitor.png}
    \caption{struttura di monitoraggio di sanet}
    \label{fig:enter-label}
\end{figure}

La messa a monitoraggio di una nuova istanza di sanet viene realizzata per mezzo del provisioner, il provisioner ottiene dal file di configurazione del sanet in questione le credenziali necessarie per contattare le API, ottiene lo stato attuale dell'installato e configura il monitoraggio sulla istanza centrale.

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=1\linewidth]{build/add_monitor_procedure.png}
%    \caption{struttura di monitoraggio di sanet}
%    \label{fig:enter-label}
%\end{figure}

% progettazione del provision

% gestione dei segreti
%Uno dei requisiti per cui e stata scelta la tecnologia a container e rendere il team NOC capace di gestire la procedura di aggiornamento massivo delle installazioni di sanet in maniera centralizzata, il container infatti consente di uniformare la procedura di installazione e aggiornamento in maniera trasparente rispetto alle decisioni prese in area NAD.
